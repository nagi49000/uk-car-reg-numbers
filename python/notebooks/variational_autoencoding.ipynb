{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a variational autoencoder\n",
    "\n",
    "Based on the ability to generate UK car registration numbers, one can build a dataset of training and test data. With a suitable vectorizer, one can go ahead and make an autoencoder.\n",
    "\n",
    "The decoder part of the variational encoder can then be used to generate new reg numbers.\n",
    "\n",
    "### References\n",
    "\n",
    "https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "\n",
    "https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras import layers\n",
    "from keras import backend\n",
    "from keras import optimizers\n",
    "import random\n",
    "\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')\n",
    "from car_reg_generator.car_reg_generator.uk_reg import UkRegGenerator\n",
    "from car_reg_generator.car_reg_generator.uk_reg import UkRegDvlaVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input vector length = 150\n",
      "actual number of validation samples = 1000\n",
      "actual number of test samples = 1000\n"
     ]
    }
   ],
   "source": [
    "n_train = 10000\n",
    "n_val = 1000\n",
    "n_test = 1000\n",
    "\n",
    "random.seed(0)\n",
    "g = UkRegGenerator()\n",
    "v = UkRegDvlaVectorizer()\n",
    "\n",
    "train_strs = [g.get_reg() for _ in range(n_train)]\n",
    "train_vecs = np.array([v.vectorize(x) for x in train_strs])\n",
    "val_strs = [g.get_reg() for _ in range(n_val)]\n",
    "val_strs = [x for x in val_strs if x not in set(train_strs)]  # sanity check\n",
    "val_vecs = np.array([v.vectorize(x) for x in val_strs])\n",
    "test_strs = [g.get_reg() for _ in range(n_test)]\n",
    "test_strs = [x for x in test_strs if x not in set(train_strs) | set(val_strs)]  # sanity check\n",
    "test_vecs = np.array([v.vectorize(x) for x in test_strs])\n",
    "\n",
    "vec_length = len(train_vecs[0])\n",
    "print('input vector length = ' + str(vec_length))\n",
    "print('actual number of validation samples = ' + str(len(val_strs)))\n",
    "print('actual number of test samples = ' + str(len(test_strs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational autoencoder\n",
    "\n",
    "Design the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 15\n",
    "\n",
    "### Define the encoder\n",
    "inputs = keras.Input(shape=(vec_length,))\n",
    "h = inputs\n",
    "h = layers.Dense(vec_length, activation='relu')(h)\n",
    "h = layers.Dense(50, activation='relu')(h)\n",
    "h = layers.Dense(25, activation='relu')(h)\n",
    "z_mean = layers.Dense(latent_dim)(h)\n",
    "z_log_sigma = layers.Dense(latent_dim)(h)\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = backend.random_normal(shape=(backend.shape(z_mean)[0], latent_dim),\n",
    "                                    mean=0., stddev=0.1)\n",
    "    return z_mean + backend.exp(z_log_sigma) * epsilon\n",
    "\n",
    "z = layers.Lambda(sampling)([z_mean, z_log_sigma])\n",
    "\n",
    "encoder = keras.Model(inputs, [z_mean, z_log_sigma, z], name='encoder')\n",
    "\n",
    "### Define the decoder\n",
    "latent_inputs = keras.Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = latent_inputs\n",
    "x = layers.Dense(25, activation='relu')(x)\n",
    "x = layers.Dense(50, activation='relu')(x)\n",
    "x = layers.Dense(vec_length, activation='relu')(x)\n",
    "x = layers.Dense(vec_length, activation='sigmoid')(x)\n",
    "outputs = x\n",
    "decoder = keras.Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = keras.Model(inputs, outputs, name='vae_mlp')\n",
    "\n",
    "### Define the loss function\n",
    "reconstruction_loss = keras.losses.binary_crossentropy(inputs, outputs)\n",
    "reconstruction_loss *= vec_length\n",
    "kl_loss = 1 + z_log_sigma - backend.square(z_mean) - backend.exp(z_log_sigma)\n",
    "kl_loss = backend.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = backend.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 28.2511 - val_loss: 26.6473\n",
      "Epoch 2/200\n",
      "1250/1250 [==============================] - 1s 932us/step - loss: 25.5185 - val_loss: 24.3848\n",
      "Epoch 3/200\n",
      "1250/1250 [==============================] - 1s 893us/step - loss: 23.2336 - val_loss: 22.4293\n",
      "Epoch 4/200\n",
      "1250/1250 [==============================] - 1s 899us/step - loss: 21.9204 - val_loss: 21.7983\n",
      "Epoch 5/200\n",
      "1250/1250 [==============================] - 1s 913us/step - loss: 21.2705 - val_loss: 21.3612\n",
      "Epoch 6/200\n",
      "1250/1250 [==============================] - 1s 900us/step - loss: 20.8199 - val_loss: 21.0122\n",
      "Epoch 7/200\n",
      "1250/1250 [==============================] - 1s 898us/step - loss: 20.4580 - val_loss: 20.5835\n",
      "Epoch 8/200\n",
      "1250/1250 [==============================] - 1s 916us/step - loss: 20.0721 - val_loss: 20.2651\n",
      "Epoch 9/200\n",
      "1250/1250 [==============================] - 1s 920us/step - loss: 19.6486 - val_loss: 19.7655\n",
      "Epoch 10/200\n",
      "1250/1250 [==============================] - 1s 917us/step - loss: 19.1509 - val_loss: 19.3671\n",
      "Epoch 11/200\n",
      "1250/1250 [==============================] - 1s 926us/step - loss: 18.6357 - val_loss: 18.7754\n",
      "Epoch 12/200\n",
      "1250/1250 [==============================] - 1s 957us/step - loss: 18.2691 - val_loss: 18.6152\n",
      "Epoch 13/200\n",
      "1250/1250 [==============================] - 1s 930us/step - loss: 17.9785 - val_loss: 18.3408\n",
      "Epoch 14/200\n",
      "1250/1250 [==============================] - 1s 919us/step - loss: 17.7683 - val_loss: 18.0159\n",
      "Epoch 15/200\n",
      "1250/1250 [==============================] - 1s 920us/step - loss: 17.5503 - val_loss: 18.0647\n",
      "Epoch 16/200\n",
      "1250/1250 [==============================] - 1s 918us/step - loss: 17.3788 - val_loss: 17.9755\n",
      "Epoch 17/200\n",
      "1250/1250 [==============================] - 1s 932us/step - loss: 17.1928 - val_loss: 17.6393\n",
      "Epoch 18/200\n",
      "1250/1250 [==============================] - 1s 926us/step - loss: 16.9967 - val_loss: 17.5742\n",
      "Epoch 19/200\n",
      "1250/1250 [==============================] - 1s 925us/step - loss: 16.8429 - val_loss: 17.3723\n",
      "Epoch 20/200\n",
      "1250/1250 [==============================] - 1s 928us/step - loss: 16.6453 - val_loss: 17.4526\n",
      "Epoch 21/200\n",
      "1250/1250 [==============================] - 1s 925us/step - loss: 16.5058 - val_loss: 17.1567\n",
      "Epoch 22/200\n",
      "1250/1250 [==============================] - 1s 930us/step - loss: 16.3472 - val_loss: 17.1056\n",
      "Epoch 23/200\n",
      "1250/1250 [==============================] - 1s 920us/step - loss: 16.1824 - val_loss: 17.0309\n",
      "Epoch 24/200\n",
      "1250/1250 [==============================] - 1s 931us/step - loss: 16.0746 - val_loss: 16.6443\n",
      "Epoch 25/200\n",
      "1250/1250 [==============================] - 1s 921us/step - loss: 15.9260 - val_loss: 16.4807\n",
      "Epoch 26/200\n",
      "1250/1250 [==============================] - 1s 933us/step - loss: 15.7529 - val_loss: 16.3857\n",
      "Epoch 27/200\n",
      "1250/1250 [==============================] - 1s 927us/step - loss: 15.6309 - val_loss: 16.2262\n",
      "Epoch 28/200\n",
      "1250/1250 [==============================] - 1s 924us/step - loss: 15.4417 - val_loss: 16.0684\n",
      "Epoch 29/200\n",
      "1250/1250 [==============================] - 1s 927us/step - loss: 15.2612 - val_loss: 16.0393\n",
      "Epoch 30/200\n",
      "1250/1250 [==============================] - 1s 927us/step - loss: 15.0512 - val_loss: 15.7153\n",
      "Epoch 31/200\n",
      "1250/1250 [==============================] - 1s 928us/step - loss: 14.7914 - val_loss: 15.7658\n",
      "Epoch 32/200\n",
      "1250/1250 [==============================] - 1s 921us/step - loss: 14.5902 - val_loss: 15.2465\n",
      "Epoch 33/200\n",
      "1250/1250 [==============================] - 1s 916us/step - loss: 14.3353 - val_loss: 15.1544\n",
      "Epoch 34/200\n",
      "1250/1250 [==============================] - 1s 931us/step - loss: 14.1443 - val_loss: 14.8026\n",
      "Epoch 35/200\n",
      "1250/1250 [==============================] - 1s 920us/step - loss: 13.9121 - val_loss: 14.5104\n",
      "Epoch 36/200\n",
      "1250/1250 [==============================] - 1s 920us/step - loss: 13.7281 - val_loss: 14.5180\n",
      "Epoch 37/200\n",
      "1250/1250 [==============================] - 1s 932us/step - loss: 13.5388 - val_loss: 14.0470\n",
      "Epoch 38/200\n",
      "1250/1250 [==============================] - 1s 927us/step - loss: 13.3906 - val_loss: 14.2563\n",
      "Epoch 39/200\n",
      "1250/1250 [==============================] - 1s 938us/step - loss: 13.1987 - val_loss: 13.8201\n",
      "Epoch 40/200\n",
      "1250/1250 [==============================] - 1s 930us/step - loss: 13.0835 - val_loss: 13.7920\n",
      "Epoch 41/200\n",
      "1250/1250 [==============================] - 1s 933us/step - loss: 12.9297 - val_loss: 13.7746\n",
      "Epoch 42/200\n",
      "1250/1250 [==============================] - 1s 931us/step - loss: 12.8334 - val_loss: 13.7092\n",
      "Epoch 43/200\n",
      "1250/1250 [==============================] - 1s 927us/step - loss: 12.6394 - val_loss: 13.5498\n",
      "Epoch 44/200\n",
      "1250/1250 [==============================] - 1s 923us/step - loss: 12.5292 - val_loss: 13.1033\n",
      "Epoch 45/200\n",
      "1250/1250 [==============================] - 1s 923us/step - loss: 12.3827 - val_loss: 13.1753\n",
      "Epoch 46/200\n",
      "1250/1250 [==============================] - 1s 923us/step - loss: 12.2624 - val_loss: 12.9133\n",
      "Epoch 47/200\n",
      "1250/1250 [==============================] - 1s 929us/step - loss: 12.1244 - val_loss: 12.8174\n",
      "Epoch 48/200\n",
      "1250/1250 [==============================] - 1s 918us/step - loss: 11.9627 - val_loss: 12.5778\n",
      "Epoch 49/200\n",
      "1250/1250 [==============================] - 1s 924us/step - loss: 11.8510 - val_loss: 12.7068\n",
      "Epoch 50/200\n",
      "1250/1250 [==============================] - 1s 932us/step - loss: 11.7074 - val_loss: 12.5247\n",
      "Epoch 51/200\n",
      "1250/1250 [==============================] - 1s 937us/step - loss: 11.5772 - val_loss: 12.2590\n",
      "Epoch 52/200\n",
      "1250/1250 [==============================] - 1s 925us/step - loss: 11.4642 - val_loss: 11.8928\n",
      "Epoch 53/200\n",
      "1250/1250 [==============================] - 1s 929us/step - loss: 11.2918 - val_loss: 11.7259\n",
      "Epoch 54/200\n",
      "1250/1250 [==============================] - 1s 933us/step - loss: 11.1463 - val_loss: 11.5661\n",
      "Epoch 55/200\n",
      "1250/1250 [==============================] - 1s 927us/step - loss: 11.0034 - val_loss: 11.7841\n",
      "Epoch 56/200\n",
      "1250/1250 [==============================] - 1s 927us/step - loss: 10.9407 - val_loss: 11.4673\n",
      "Epoch 57/200\n",
      "1250/1250 [==============================] - 1s 935us/step - loss: 10.8074 - val_loss: 11.6826\n",
      "Epoch 58/200\n",
      "1250/1250 [==============================] - 1s 936us/step - loss: 10.7525 - val_loss: 11.1253\n",
      "Epoch 59/200\n",
      "1250/1250 [==============================] - 1s 924us/step - loss: 10.6674 - val_loss: 11.3228\n",
      "Epoch 60/200\n",
      "1250/1250 [==============================] - 1s 932us/step - loss: 10.5892 - val_loss: 11.3816\n",
      "Epoch 61/200\n",
      "1250/1250 [==============================] - 1s 933us/step - loss: 10.5091 - val_loss: 11.4343\n",
      "Epoch 62/200\n",
      "1250/1250 [==============================] - 1s 925us/step - loss: 10.4571 - val_loss: 11.0209\n",
      "Epoch 63/200\n",
      "1250/1250 [==============================] - 1s 926us/step - loss: 10.3886 - val_loss: 11.5867\n",
      "Epoch 64/200\n",
      "1250/1250 [==============================] - 1s 931us/step - loss: 10.3426 - val_loss: 10.9575\n",
      "Epoch 65/200\n",
      "1250/1250 [==============================] - 1s 929us/step - loss: 10.2292 - val_loss: 11.3211\n",
      "Epoch 66/200\n",
      "1250/1250 [==============================] - 1s 934us/step - loss: 10.2025 - val_loss: 10.7446\n",
      "Epoch 67/200\n",
      "1250/1250 [==============================] - 1s 931us/step - loss: 10.1248 - val_loss: 10.7042\n",
      "Epoch 68/200\n",
      "1250/1250 [==============================] - 1s 929us/step - loss: 10.0674 - val_loss: 10.8903\n",
      "Epoch 69/200\n",
      "1250/1250 [==============================] - 1s 922us/step - loss: 10.0458 - val_loss: 10.5728\n",
      "Epoch 70/200\n",
      "1250/1250 [==============================] - 1s 928us/step - loss: 9.9948 - val_loss: 10.5116\n",
      "Epoch 71/200\n",
      "1250/1250 [==============================] - 1s 933us/step - loss: 9.9795 - val_loss: 10.5427\n",
      "Epoch 72/200\n",
      "1250/1250 [==============================] - 1s 927us/step - loss: 9.8849 - val_loss: 10.3433\n",
      "Epoch 73/200\n",
      "1250/1250 [==============================] - 1s 930us/step - loss: 9.8329 - val_loss: 10.7808\n",
      "Epoch 74/200\n",
      "1250/1250 [==============================] - 1s 931us/step - loss: 9.8363 - val_loss: 10.3776\n",
      "Epoch 75/200\n",
      "1250/1250 [==============================] - 1s 922us/step - loss: 9.7069 - val_loss: 10.6309\n",
      "Epoch 76/200\n",
      "1250/1250 [==============================] - 1s 928us/step - loss: 9.7128 - val_loss: 10.2806\n",
      "Epoch 77/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 1s 888us/step - loss: 9.6320 - val_loss: 10.6184\n",
      "Epoch 78/200\n",
      "1250/1250 [==============================] - 1s 873us/step - loss: 9.6522 - val_loss: 10.1244\n",
      "Epoch 79/200\n",
      "1250/1250 [==============================] - 1s 884us/step - loss: 9.6017 - val_loss: 10.3266\n",
      "Epoch 80/200\n",
      "1250/1250 [==============================] - 1s 876us/step - loss: 9.4617 - val_loss: 10.1652\n",
      "Epoch 81/200\n",
      "1250/1250 [==============================] - 1s 883us/step - loss: 9.4050 - val_loss: 10.2698\n",
      "Epoch 82/200\n",
      "1250/1250 [==============================] - 1s 879us/step - loss: 9.3269 - val_loss: 10.0110\n",
      "Epoch 83/200\n",
      "1250/1250 [==============================] - 1s 873us/step - loss: 9.2717 - val_loss: 10.2363\n",
      "Epoch 84/200\n",
      "1250/1250 [==============================] - 1s 880us/step - loss: 9.1811 - val_loss: 10.6479\n",
      "Epoch 85/200\n",
      "1250/1250 [==============================] - 1s 880us/step - loss: 9.0823 - val_loss: 10.0517\n",
      "Epoch 86/200\n",
      "1250/1250 [==============================] - 1s 873us/step - loss: 9.0380 - val_loss: 10.5410\n",
      "Epoch 87/200\n",
      "1250/1250 [==============================] - 1s 879us/step - loss: 8.9871 - val_loss: 9.6669\n",
      "Epoch 88/200\n",
      "1250/1250 [==============================] - 1s 894us/step - loss: 8.9198 - val_loss: 9.7753\n",
      "Epoch 89/200\n",
      "1250/1250 [==============================] - 1s 880us/step - loss: 8.8656 - val_loss: 10.2106\n",
      "Epoch 90/200\n",
      "1250/1250 [==============================] - 1s 875us/step - loss: 8.7932 - val_loss: 9.5457\n",
      "Epoch 91/200\n",
      "1250/1250 [==============================] - 1s 883us/step - loss: 8.7557 - val_loss: 9.2159\n",
      "Epoch 92/200\n",
      "1250/1250 [==============================] - 1s 877us/step - loss: 8.6463 - val_loss: 9.7862\n",
      "Epoch 93/200\n",
      "1250/1250 [==============================] - 1s 881us/step - loss: 8.5841 - val_loss: 9.3541\n",
      "Epoch 94/200\n",
      "1250/1250 [==============================] - 1s 877us/step - loss: 8.5658 - val_loss: 9.2845\n",
      "Epoch 95/200\n",
      "1250/1250 [==============================] - 1s 884us/step - loss: 8.4537 - val_loss: 9.2109\n",
      "Epoch 96/200\n",
      "1250/1250 [==============================] - 1s 896us/step - loss: 8.4376 - val_loss: 9.0221\n",
      "Epoch 97/200\n",
      "1250/1250 [==============================] - 1s 890us/step - loss: 8.3405 - val_loss: 9.0919\n",
      "Epoch 98/200\n",
      "1250/1250 [==============================] - 1s 884us/step - loss: 8.2912 - val_loss: 8.7238\n",
      "Epoch 99/200\n",
      "1250/1250 [==============================] - 1s 883us/step - loss: 8.2303 - val_loss: 9.2198\n",
      "Epoch 100/200\n",
      "1250/1250 [==============================] - 1s 880us/step - loss: 8.1538 - val_loss: 9.0197\n",
      "Epoch 101/200\n",
      "1250/1250 [==============================] - 1s 884us/step - loss: 8.1218 - val_loss: 9.0115\n",
      "Epoch 102/200\n",
      "1250/1250 [==============================] - 1s 886us/step - loss: 8.0752 - val_loss: 8.7948\n",
      "Epoch 103/200\n",
      "1250/1250 [==============================] - 1s 879us/step - loss: 8.0380 - val_loss: 9.0619\n",
      "Epoch 104/200\n",
      "1250/1250 [==============================] - 1s 887us/step - loss: 7.9369 - val_loss: 8.7591\n",
      "Epoch 105/200\n",
      "1250/1250 [==============================] - 1s 882us/step - loss: 7.9487 - val_loss: 8.5461\n",
      "Epoch 106/200\n",
      "1250/1250 [==============================] - 1s 881us/step - loss: 7.8927 - val_loss: 9.2724\n",
      "Epoch 107/200\n",
      "1250/1250 [==============================] - 1s 887us/step - loss: 7.8010 - val_loss: 8.5083\n",
      "Epoch 108/200\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 7.7564 - val_loss: 8.6739\n",
      "Epoch 109/200\n",
      "1250/1250 [==============================] - 1s 887us/step - loss: 7.6826 - val_loss: 9.0446\n",
      "Epoch 110/200\n",
      "1250/1250 [==============================] - 1s 881us/step - loss: 7.5840 - val_loss: 8.6874\n",
      "Epoch 111/200\n",
      "1250/1250 [==============================] - 1s 883us/step - loss: 7.5215 - val_loss: 8.2125\n",
      "Epoch 112/200\n",
      "1250/1250 [==============================] - 1s 880us/step - loss: 7.4962 - val_loss: 8.6575\n",
      "Epoch 113/200\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 7.4643 - val_loss: 8.0911\n",
      "Epoch 114/200\n",
      "1250/1250 [==============================] - 1s 881us/step - loss: 7.3940 - val_loss: 8.3340\n",
      "Epoch 115/200\n",
      "1250/1250 [==============================] - 1s 879us/step - loss: 7.3025 - val_loss: 7.9720\n",
      "Epoch 116/200\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 7.1735 - val_loss: 7.6405\n",
      "Epoch 117/200\n",
      "1250/1250 [==============================] - 1s 876us/step - loss: 7.2124 - val_loss: 7.7234\n",
      "Epoch 118/200\n",
      "1250/1250 [==============================] - 1s 881us/step - loss: 7.1226 - val_loss: 8.1139\n",
      "Epoch 119/200\n",
      "1250/1250 [==============================] - 1s 880us/step - loss: 7.0178 - val_loss: 7.9284\n",
      "Epoch 120/200\n",
      "1250/1250 [==============================] - 1s 882us/step - loss: 7.0275 - val_loss: 7.9604\n",
      "Epoch 121/200\n",
      "1250/1250 [==============================] - 1s 886us/step - loss: 6.9015 - val_loss: 7.5614\n",
      "Epoch 122/200\n",
      "1250/1250 [==============================] - 1s 882us/step - loss: 6.9070 - val_loss: 7.8702\n",
      "Epoch 123/200\n",
      "1250/1250 [==============================] - 1s 888us/step - loss: 6.8370 - val_loss: 7.6811\n",
      "Epoch 124/200\n",
      "1250/1250 [==============================] - 1s 879us/step - loss: 6.7406 - val_loss: 7.8959\n",
      "Epoch 125/200\n",
      "1250/1250 [==============================] - 1s 880us/step - loss: 6.7600 - val_loss: 7.8195\n",
      "Epoch 126/200\n",
      "1250/1250 [==============================] - 1s 883us/step - loss: 6.7460 - val_loss: 8.1289\n",
      "Epoch 127/200\n",
      "1250/1250 [==============================] - 1s 877us/step - loss: 6.6653 - val_loss: 7.5437\n",
      "Epoch 128/200\n",
      "1250/1250 [==============================] - 1s 886us/step - loss: 6.6772 - val_loss: 7.1004\n",
      "Epoch 129/200\n",
      "1250/1250 [==============================] - 1s 876us/step - loss: 6.6933 - val_loss: 7.2435\n",
      "Epoch 130/200\n",
      "1250/1250 [==============================] - 1s 880us/step - loss: 6.5936 - val_loss: 7.3828\n",
      "Epoch 131/200\n",
      "1250/1250 [==============================] - 1s 880us/step - loss: 6.5630 - val_loss: 7.5920\n",
      "Epoch 132/200\n",
      "1250/1250 [==============================] - 1s 880us/step - loss: 6.4852 - val_loss: 8.5453\n",
      "Epoch 133/200\n",
      "1250/1250 [==============================] - 1s 885us/step - loss: 6.5250 - val_loss: 7.6361\n",
      "Epoch 134/200\n",
      "1250/1250 [==============================] - 1s 881us/step - loss: 6.4784 - val_loss: 7.2130\n",
      "Epoch 135/200\n",
      "1250/1250 [==============================] - 1s 892us/step - loss: 6.4588 - val_loss: 7.0491\n",
      "Epoch 136/200\n",
      "1250/1250 [==============================] - 1s 892us/step - loss: 6.3763 - val_loss: 7.2871\n",
      "Epoch 137/200\n",
      "1250/1250 [==============================] - 1s 885us/step - loss: 6.4007 - val_loss: 6.7500\n",
      "Epoch 138/200\n",
      "1250/1250 [==============================] - 1s 883us/step - loss: 6.3320 - val_loss: 6.9394\n",
      "Epoch 139/200\n",
      "1250/1250 [==============================] - 1s 882us/step - loss: 6.3591 - val_loss: 7.1390\n",
      "Epoch 140/200\n",
      "1250/1250 [==============================] - 1s 888us/step - loss: 6.3196 - val_loss: 6.9521\n",
      "Epoch 141/200\n",
      "1250/1250 [==============================] - 1s 882us/step - loss: 6.3334 - val_loss: 7.1429\n",
      "Epoch 142/200\n",
      "1250/1250 [==============================] - 1s 886us/step - loss: 6.3008 - val_loss: 6.9149\n",
      "Epoch 143/200\n",
      "1250/1250 [==============================] - 1s 881us/step - loss: 6.2310 - val_loss: 7.3519\n",
      "Epoch 144/200\n",
      "1250/1250 [==============================] - 1s 888us/step - loss: 6.1616 - val_loss: 6.8529\n",
      "Epoch 145/200\n",
      "1250/1250 [==============================] - 1s 886us/step - loss: 6.2162 - val_loss: 6.5974\n",
      "Epoch 146/200\n",
      "1250/1250 [==============================] - 1s 889us/step - loss: 6.1947 - val_loss: 6.5350\n",
      "Epoch 147/200\n",
      "1250/1250 [==============================] - 1s 885us/step - loss: 6.1569 - val_loss: 6.6625\n",
      "Epoch 148/200\n",
      "1250/1250 [==============================] - 1s 885us/step - loss: 6.1651 - val_loss: 7.4663\n",
      "Epoch 149/200\n",
      "1250/1250 [==============================] - 1s 891us/step - loss: 6.1179 - val_loss: 6.5569\n",
      "Epoch 150/200\n",
      "1250/1250 [==============================] - 1s 885us/step - loss: 6.0417 - val_loss: 6.7537\n",
      "Epoch 151/200\n",
      "1250/1250 [==============================] - 1s 883us/step - loss: 6.0618 - val_loss: 6.8622\n",
      "Epoch 152/200\n",
      "1250/1250 [==============================] - 1s 887us/step - loss: 6.0281 - val_loss: 6.5727\n",
      "Epoch 153/200\n",
      "1250/1250 [==============================] - 1s 888us/step - loss: 6.0685 - val_loss: 7.0993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/200\n",
      "1250/1250 [==============================] - 1s 879us/step - loss: 5.9790 - val_loss: 6.6265\n",
      "Epoch 155/200\n",
      "1250/1250 [==============================] - 1s 875us/step - loss: 6.0431 - val_loss: 6.8019\n",
      "Epoch 156/200\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 5.9492 - val_loss: 6.5878\n",
      "Epoch 157/200\n",
      "1250/1250 [==============================] - 1s 877us/step - loss: 5.9892 - val_loss: 6.2476\n",
      "Epoch 158/200\n",
      "1250/1250 [==============================] - 1s 880us/step - loss: 5.9317 - val_loss: 6.5385\n",
      "Epoch 159/200\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 5.9667 - val_loss: 6.6790\n",
      "Epoch 160/200\n",
      "1250/1250 [==============================] - 1s 874us/step - loss: 5.9050 - val_loss: 6.8119\n",
      "Epoch 161/200\n",
      "1250/1250 [==============================] - 1s 882us/step - loss: 5.9187 - val_loss: 6.5284\n",
      "Epoch 162/200\n",
      "1250/1250 [==============================] - 1s 888us/step - loss: 5.8872 - val_loss: 7.3363\n",
      "Epoch 163/200\n",
      "1250/1250 [==============================] - 1s 877us/step - loss: 5.8407 - val_loss: 6.8315\n",
      "Epoch 164/200\n",
      "1250/1250 [==============================] - 1s 875us/step - loss: 5.8896 - val_loss: 6.4839\n",
      "Epoch 165/200\n",
      "1250/1250 [==============================] - 1s 882us/step - loss: 5.8418 - val_loss: 6.5668\n",
      "Epoch 166/200\n",
      "1250/1250 [==============================] - 1s 877us/step - loss: 5.7877 - val_loss: 6.2767\n",
      "Epoch 167/200\n",
      "1250/1250 [==============================] - 1s 879us/step - loss: 5.7442 - val_loss: 6.6075\n",
      "Epoch 168/200\n",
      "1250/1250 [==============================] - 1s 876us/step - loss: 5.7974 - val_loss: 6.2766\n",
      "Epoch 169/200\n",
      "1250/1250 [==============================] - 1s 874us/step - loss: 5.7346 - val_loss: 6.3365\n",
      "Epoch 170/200\n",
      "1250/1250 [==============================] - 1s 879us/step - loss: 5.7771 - val_loss: 6.4950\n",
      "Epoch 171/200\n",
      "1250/1250 [==============================] - 1s 883us/step - loss: 5.7524 - val_loss: 6.2628\n",
      "Epoch 172/200\n",
      "1250/1250 [==============================] - 1s 882us/step - loss: 5.6964 - val_loss: 6.4055\n",
      "Epoch 173/200\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 5.7055 - val_loss: 6.1420\n",
      "Epoch 174/200\n",
      "1250/1250 [==============================] - 1s 882us/step - loss: 5.7153 - val_loss: 6.5126\n",
      "Epoch 175/200\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 5.6512 - val_loss: 6.3304\n",
      "Epoch 176/200\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 5.6399 - val_loss: 6.2295\n",
      "Epoch 177/200\n",
      "1250/1250 [==============================] - 1s 883us/step - loss: 5.6704 - val_loss: 6.2889\n",
      "Epoch 178/200\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 5.6581 - val_loss: 6.1843\n",
      "Epoch 179/200\n",
      "1250/1250 [==============================] - 1s 883us/step - loss: 5.5773 - val_loss: 7.0142\n",
      "Epoch 180/200\n",
      "1250/1250 [==============================] - 1s 876us/step - loss: 5.5824 - val_loss: 6.2668\n",
      "Epoch 181/200\n",
      "1250/1250 [==============================] - 1s 883us/step - loss: 5.5569 - val_loss: 6.0621\n",
      "Epoch 182/200\n",
      "1250/1250 [==============================] - 1s 882us/step - loss: 5.5758 - val_loss: 6.0077\n",
      "Epoch 183/200\n",
      "1250/1250 [==============================] - 1s 884us/step - loss: 5.5650 - val_loss: 6.4379\n",
      "Epoch 184/200\n",
      "1250/1250 [==============================] - 1s 879us/step - loss: 5.5168 - val_loss: 6.1495\n",
      "Epoch 185/200\n",
      "1250/1250 [==============================] - 1s 887us/step - loss: 5.5396 - val_loss: 5.9834\n",
      "Epoch 186/200\n",
      "1250/1250 [==============================] - 1s 883us/step - loss: 5.5254 - val_loss: 6.0471\n",
      "Epoch 187/200\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 5.4548 - val_loss: 6.1567\n",
      "Epoch 188/200\n",
      "1250/1250 [==============================] - 1s 891us/step - loss: 5.4750 - val_loss: 6.1296\n",
      "Epoch 189/200\n",
      "1250/1250 [==============================] - 1s 880us/step - loss: 5.4738 - val_loss: 6.8556\n",
      "Epoch 190/200\n",
      "1250/1250 [==============================] - 1s 883us/step - loss: 5.4161 - val_loss: 6.6852\n",
      "Epoch 191/200\n",
      "1250/1250 [==============================] - 1s 880us/step - loss: 5.4270 - val_loss: 5.9011\n",
      "Epoch 192/200\n",
      "1250/1250 [==============================] - 1s 886us/step - loss: 5.4916 - val_loss: 5.9064\n",
      "Epoch 193/200\n",
      "1250/1250 [==============================] - 1s 881us/step - loss: 5.4361 - val_loss: 5.8172\n",
      "Epoch 194/200\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 5.4358 - val_loss: 6.0158\n",
      "Epoch 195/200\n",
      "1250/1250 [==============================] - 1s 877us/step - loss: 5.3232 - val_loss: 6.3543\n",
      "Epoch 196/200\n",
      "1250/1250 [==============================] - 1s 883us/step - loss: 5.3761 - val_loss: 6.1969\n",
      "Epoch 197/200\n",
      "1250/1250 [==============================] - 1s 881us/step - loss: 5.3670 - val_loss: 6.0228\n",
      "Epoch 198/200\n",
      "1250/1250 [==============================] - 1s 882us/step - loss: 5.3178 - val_loss: 6.0307\n",
      "Epoch 199/200\n",
      "1250/1250 [==============================] - 1s 889us/step - loss: 5.3474 - val_loss: 5.9018\n",
      "Epoch 200/200\n",
      "1250/1250 [==============================] - 1s 882us/step - loss: 5.3105 - val_loss: 6.2513\n",
      "Epoch 1/200\n",
      "1250/1250 [==============================] - 1s 971us/step - loss: 4.5267 - val_loss: 5.0856\n",
      "Epoch 2/200\n",
      "1250/1250 [==============================] - 1s 880us/step - loss: 4.3902 - val_loss: 4.9579\n",
      "Epoch 3/200\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 4.3424 - val_loss: 4.9411\n",
      "Epoch 4/200\n",
      "1250/1250 [==============================] - 1s 892us/step - loss: 4.3331 - val_loss: 4.9280\n",
      "Epoch 5/200\n",
      "1250/1250 [==============================] - 1s 876us/step - loss: 4.3040 - val_loss: 4.8847\n",
      "Epoch 6/200\n",
      "1250/1250 [==============================] - 1s 882us/step - loss: 4.2913 - val_loss: 4.9734\n",
      "Epoch 7/200\n",
      "1250/1250 [==============================] - 1s 875us/step - loss: 4.3022 - val_loss: 4.7599\n",
      "Epoch 8/200\n",
      "1250/1250 [==============================] - 1s 877us/step - loss: 4.2673 - val_loss: 4.8288\n",
      "Epoch 9/200\n",
      "1250/1250 [==============================] - 1s 875us/step - loss: 4.2757 - val_loss: 5.0139\n",
      "Epoch 10/200\n",
      "1250/1250 [==============================] - 1s 873us/step - loss: 4.2648 - val_loss: 4.8366\n",
      "Epoch 11/200\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 4.2607 - val_loss: 4.9286\n",
      "Epoch 12/200\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 4.2833 - val_loss: 4.8606\n",
      "Epoch 13/200\n",
      "1250/1250 [==============================] - 1s 876us/step - loss: 4.2299 - val_loss: 4.8701\n",
      "Epoch 14/200\n",
      "1250/1250 [==============================] - 1s 879us/step - loss: 4.2204 - val_loss: 4.8179\n",
      "Epoch 15/200\n",
      "1250/1250 [==============================] - 1s 881us/step - loss: 4.2324 - val_loss: 4.7939\n",
      "Epoch 16/200\n",
      "1250/1250 [==============================] - 1s 877us/step - loss: 4.2437 - val_loss: 4.8202\n",
      "Epoch 17/200\n",
      "1250/1250 [==============================] - 1s 879us/step - loss: 4.2258 - val_loss: 4.9179\n",
      "Epoch 18/200\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 4.2217 - val_loss: 4.7663\n",
      "Epoch 19/200\n",
      "1250/1250 [==============================] - 1s 886us/step - loss: 4.2352 - val_loss: 4.9366\n",
      "Epoch 20/200\n",
      "1250/1250 [==============================] - 1s 887us/step - loss: 4.2148 - val_loss: 4.7257\n",
      "Epoch 21/200\n",
      "1250/1250 [==============================] - 1s 880us/step - loss: 4.2047 - val_loss: 4.7836\n",
      "Epoch 22/200\n",
      "1250/1250 [==============================] - 1s 879us/step - loss: 4.1793 - val_loss: 4.8794\n",
      "Epoch 23/200\n",
      "1250/1250 [==============================] - 1s 880us/step - loss: 4.2258 - val_loss: 4.8197\n",
      "Epoch 24/200\n",
      "1250/1250 [==============================] - 1s 881us/step - loss: 4.1886 - val_loss: 4.9397\n",
      "Epoch 25/200\n",
      "1250/1250 [==============================] - 1s 885us/step - loss: 4.1774 - val_loss: 4.7271\n",
      "Epoch 26/200\n",
      "1250/1250 [==============================] - 1s 879us/step - loss: 4.1855 - val_loss: 4.8021\n",
      "Epoch 27/200\n",
      "1250/1250 [==============================] - 1s 877us/step - loss: 4.2198 - val_loss: 4.7214\n",
      "Epoch 28/200\n",
      "1250/1250 [==============================] - 1s 885us/step - loss: 4.1870 - val_loss: 4.7796\n",
      "Epoch 29/200\n",
      "1250/1250 [==============================] - 1s 885us/step - loss: 4.1702 - val_loss: 4.7494\n",
      "Epoch 30/200\n",
      "1250/1250 [==============================] - 1s 876us/step - loss: 4.1526 - val_loss: 4.7717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/200\n",
      "1250/1250 [==============================] - 1s 871us/step - loss: 4.1442 - val_loss: 4.8642\n",
      "Epoch 32/200\n",
      "1250/1250 [==============================] - 1s 874us/step - loss: 4.1567 - val_loss: 4.7782\n",
      "Epoch 33/200\n",
      "1250/1250 [==============================] - 1s 870us/step - loss: 4.1684 - val_loss: 4.6957\n",
      "Epoch 34/200\n",
      "1250/1250 [==============================] - 1s 870us/step - loss: 4.1590 - val_loss: 4.7626\n",
      "Epoch 35/200\n",
      "1250/1250 [==============================] - 1s 871us/step - loss: 4.1495 - val_loss: 4.7172\n",
      "Epoch 36/200\n",
      "1250/1250 [==============================] - 1s 876us/step - loss: 4.1427 - val_loss: 4.9358\n",
      "Epoch 37/200\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 4.1281 - val_loss: 4.8064\n",
      "Epoch 38/200\n",
      "1250/1250 [==============================] - 1s 872us/step - loss: 4.1060 - val_loss: 4.7208\n",
      "Epoch 39/200\n",
      "1250/1250 [==============================] - 1s 876us/step - loss: 4.1371 - val_loss: 4.9101\n",
      "Epoch 40/200\n",
      "1250/1250 [==============================] - 1s 872us/step - loss: 4.1157 - val_loss: 4.5888\n",
      "Epoch 41/200\n",
      "1250/1250 [==============================] - 1s 872us/step - loss: 4.1394 - val_loss: 4.7156\n",
      "Epoch 42/200\n",
      "1250/1250 [==============================] - 1s 877us/step - loss: 4.1299 - val_loss: 4.7445\n",
      "Epoch 43/200\n",
      "1250/1250 [==============================] - 1s 876us/step - loss: 4.1036 - val_loss: 4.8700\n",
      "Epoch 44/200\n",
      "1250/1250 [==============================] - 1s 874us/step - loss: 4.1541 - val_loss: 4.6243\n",
      "Epoch 45/200\n",
      "1250/1250 [==============================] - 1s 869us/step - loss: 4.1036 - val_loss: 4.7440\n",
      "Epoch 46/200\n",
      "1250/1250 [==============================] - 1s 879us/step - loss: 4.1064 - val_loss: 4.7571\n",
      "Epoch 47/200\n",
      "1250/1250 [==============================] - 1s 876us/step - loss: 4.1190 - val_loss: 4.6344\n",
      "Epoch 48/200\n",
      "1250/1250 [==============================] - 1s 875us/step - loss: 4.0965 - val_loss: 4.6586\n",
      "Epoch 49/200\n",
      "1250/1250 [==============================] - 1s 876us/step - loss: 4.1164 - val_loss: 4.5693\n",
      "Epoch 50/200\n",
      "1250/1250 [==============================] - 1s 879us/step - loss: 4.0781 - val_loss: 4.6907\n",
      "Epoch 51/200\n",
      "1250/1250 [==============================] - 1s 875us/step - loss: 4.1019 - val_loss: 4.7038\n",
      "Epoch 52/200\n",
      "1250/1250 [==============================] - 1s 885us/step - loss: 4.1062 - val_loss: 4.6709\n",
      "Epoch 53/200\n",
      "1250/1250 [==============================] - 1s 875us/step - loss: 4.1264 - val_loss: 4.7705\n",
      "Epoch 54/200\n",
      "1250/1250 [==============================] - 1s 873us/step - loss: 4.0783 - val_loss: 4.7093\n",
      "Epoch 55/200\n",
      "1250/1250 [==============================] - 1s 874us/step - loss: 4.0645 - val_loss: 4.6708\n",
      "Epoch 56/200\n",
      "1250/1250 [==============================] - 1s 874us/step - loss: 4.0736 - val_loss: 4.6256\n",
      "Epoch 57/200\n",
      "1250/1250 [==============================] - 1s 874us/step - loss: 4.0721 - val_loss: 4.7784\n",
      "Epoch 58/200\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 4.0968 - val_loss: 4.7368\n",
      "Epoch 59/200\n",
      "1250/1250 [==============================] - 1s 873us/step - loss: 4.0749 - val_loss: 4.7234\n",
      "Epoch 60/200\n",
      "1250/1250 [==============================] - 1s 873us/step - loss: 4.0703 - val_loss: 4.6053\n",
      "Epoch 61/200\n",
      "1250/1250 [==============================] - 1s 877us/step - loss: 4.0859 - val_loss: 4.6805\n",
      "Epoch 62/200\n",
      "1250/1250 [==============================] - 1s 875us/step - loss: 4.0729 - val_loss: 4.7228\n",
      "Epoch 63/200\n",
      "1250/1250 [==============================] - 1s 875us/step - loss: 4.0502 - val_loss: 4.7138\n",
      "Epoch 64/200\n",
      "1250/1250 [==============================] - 1s 871us/step - loss: 4.0492 - val_loss: 4.5797\n",
      "Epoch 65/200\n",
      "1250/1250 [==============================] - 1s 876us/step - loss: 4.0798 - val_loss: 4.5981\n",
      "Epoch 66/200\n",
      "1250/1250 [==============================] - 1s 876us/step - loss: 4.0645 - val_loss: 4.7238\n",
      "Epoch 67/200\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 4.0293 - val_loss: 4.5754\n",
      "Epoch 68/200\n",
      "1250/1250 [==============================] - 1s 870us/step - loss: 4.0496 - val_loss: 4.7323\n",
      "Epoch 69/200\n",
      "1250/1250 [==============================] - 1s 872us/step - loss: 4.0674 - val_loss: 4.6040\n",
      "Epoch 70/200\n",
      "1250/1250 [==============================] - 1s 876us/step - loss: 4.0398 - val_loss: 4.5412\n",
      "Epoch 71/200\n",
      "1250/1250 [==============================] - 1s 876us/step - loss: 4.0263 - val_loss: 4.6478\n",
      "Epoch 72/200\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 4.0517 - val_loss: 4.5446\n",
      "Epoch 73/200\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 4.0308 - val_loss: 4.7833\n",
      "Epoch 74/200\n",
      "1250/1250 [==============================] - 1s 882us/step - loss: 4.0324 - val_loss: 4.6271\n",
      "Epoch 75/200\n",
      "1250/1250 [==============================] - 1s 876us/step - loss: 4.0390 - val_loss: 4.7564\n",
      "Epoch 76/200\n",
      "1250/1250 [==============================] - 1s 877us/step - loss: 4.0190 - val_loss: 4.6429\n",
      "Epoch 77/200\n",
      "1250/1250 [==============================] - 1s 877us/step - loss: 4.0072 - val_loss: 4.6176\n",
      "Epoch 78/200\n",
      "1250/1250 [==============================] - 1s 872us/step - loss: 4.0194 - val_loss: 4.6123\n",
      "Epoch 79/200\n",
      "1250/1250 [==============================] - 1s 873us/step - loss: 4.0454 - val_loss: 4.6948\n",
      "Epoch 80/200\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 4.0129 - val_loss: 4.5360\n",
      "Epoch 81/200\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 4.0489 - val_loss: 4.6285\n",
      "Epoch 82/200\n",
      "1250/1250 [==============================] - 1s 885us/step - loss: 4.0340 - val_loss: 4.6597\n",
      "Epoch 83/200\n",
      "1250/1250 [==============================] - 1s 877us/step - loss: 3.9922 - val_loss: 4.5666\n",
      "Epoch 84/200\n",
      "1250/1250 [==============================] - 1s 871us/step - loss: 4.0112 - val_loss: 4.7057\n",
      "Epoch 85/200\n",
      "1250/1250 [==============================] - 1s 881us/step - loss: 4.0187 - val_loss: 4.6172\n",
      "Epoch 86/200\n",
      "1250/1250 [==============================] - 1s 876us/step - loss: 4.0072 - val_loss: 4.6377\n",
      "Epoch 87/200\n",
      "1250/1250 [==============================] - 1s 889us/step - loss: 4.0250 - val_loss: 4.5520\n",
      "Epoch 88/200\n",
      "1250/1250 [==============================] - 1s 879us/step - loss: 3.9943 - val_loss: 4.5224\n",
      "Epoch 89/200\n",
      "1250/1250 [==============================] - 1s 880us/step - loss: 3.9924 - val_loss: 4.5643\n",
      "Epoch 90/200\n",
      "1250/1250 [==============================] - 1s 874us/step - loss: 4.0104 - val_loss: 4.6121\n",
      "Epoch 91/200\n",
      "1250/1250 [==============================] - 1s 881us/step - loss: 3.9799 - val_loss: 4.6213\n",
      "Epoch 92/200\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 3.9902 - val_loss: 4.5816\n",
      "Epoch 93/200\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 3.9921 - val_loss: 4.6543\n",
      "Epoch 94/200\n",
      "1250/1250 [==============================] - 1s 874us/step - loss: 3.9945 - val_loss: 4.5477\n",
      "Epoch 95/200\n",
      "1250/1250 [==============================] - 1s 882us/step - loss: 3.9908 - val_loss: 4.6332\n",
      "Epoch 96/200\n",
      "1250/1250 [==============================] - 1s 880us/step - loss: 3.9701 - val_loss: 4.5999\n",
      "Epoch 97/200\n",
      "1250/1250 [==============================] - 1s 879us/step - loss: 3.9796 - val_loss: 4.7254\n",
      "Epoch 98/200\n",
      "1250/1250 [==============================] - 1s 877us/step - loss: 3.9972 - val_loss: 4.7157\n",
      "Epoch 99/200\n",
      "1250/1250 [==============================] - 1s 875us/step - loss: 3.9927 - val_loss: 4.5925\n",
      "Epoch 100/200\n",
      "1250/1250 [==============================] - 1s 877us/step - loss: 3.9740 - val_loss: 4.4881\n",
      "Epoch 101/200\n",
      "1250/1250 [==============================] - 1s 872us/step - loss: 3.9807 - val_loss: 4.6963\n",
      "Epoch 102/200\n",
      "1250/1250 [==============================] - 1s 876us/step - loss: 3.9711 - val_loss: 4.6219\n",
      "Epoch 103/200\n",
      "1250/1250 [==============================] - 1s 877us/step - loss: 3.9637 - val_loss: 4.5630\n",
      "Epoch 104/200\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 3.9658 - val_loss: 4.5306\n",
      "Epoch 105/200\n",
      "1250/1250 [==============================] - 1s 879us/step - loss: 3.9797 - val_loss: 4.6060\n",
      "Epoch 106/200\n",
      "1250/1250 [==============================] - 1s 882us/step - loss: 3.9527 - val_loss: 4.7666\n",
      "Epoch 107/200\n",
      "1250/1250 [==============================] - 1s 879us/step - loss: 3.9509 - val_loss: 4.5684\n",
      "Epoch 108/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 1s 876us/step - loss: 3.9492 - val_loss: 4.5706\n",
      "Epoch 109/200\n",
      "1250/1250 [==============================] - 1s 872us/step - loss: 3.9601 - val_loss: 4.5075\n",
      "Epoch 110/200\n",
      "1250/1250 [==============================] - 1s 874us/step - loss: 3.9452 - val_loss: 4.5993\n",
      "Epoch 111/200\n",
      "1250/1250 [==============================] - 1s 867us/step - loss: 3.9826 - val_loss: 4.4959\n",
      "Epoch 112/200\n",
      "1250/1250 [==============================] - 1s 875us/step - loss: 3.9436 - val_loss: 4.5265\n",
      "Epoch 113/200\n",
      "1250/1250 [==============================] - 1s 887us/step - loss: 3.9347 - val_loss: 4.5558\n",
      "Epoch 114/200\n",
      "1250/1250 [==============================] - 1s 872us/step - loss: 3.9333 - val_loss: 4.5339\n",
      "Epoch 115/200\n",
      "1250/1250 [==============================] - 1s 884us/step - loss: 3.9489 - val_loss: 4.5876\n",
      "Epoch 116/200\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 3.9453 - val_loss: 4.5282\n",
      "Epoch 117/200\n",
      "1250/1250 [==============================] - 1s 875us/step - loss: 3.9459 - val_loss: 4.4814\n",
      "Epoch 118/200\n",
      "1250/1250 [==============================] - 1s 877us/step - loss: 3.9354 - val_loss: 4.4877\n",
      "Epoch 119/200\n",
      "1250/1250 [==============================] - 1s 868us/step - loss: 3.9503 - val_loss: 4.4884\n",
      "Epoch 120/200\n",
      "1250/1250 [==============================] - 1s 877us/step - loss: 3.9114 - val_loss: 4.5530\n",
      "Epoch 121/200\n",
      "1250/1250 [==============================] - 1s 874us/step - loss: 3.9348 - val_loss: 4.5184\n",
      "Epoch 122/200\n",
      "1250/1250 [==============================] - 1s 874us/step - loss: 3.9208 - val_loss: 4.5729\n",
      "Epoch 123/200\n",
      "1250/1250 [==============================] - 1s 869us/step - loss: 3.9223 - val_loss: 4.5882\n",
      "Epoch 124/200\n",
      "1250/1250 [==============================] - 1s 877us/step - loss: 3.9442 - val_loss: 4.5868\n",
      "Epoch 125/200\n",
      "1250/1250 [==============================] - 1s 872us/step - loss: 3.9182 - val_loss: 4.5296\n",
      "Epoch 126/200\n",
      "1250/1250 [==============================] - 1s 880us/step - loss: 3.9251 - val_loss: 4.6838\n",
      "Epoch 127/200\n",
      "1250/1250 [==============================] - 1s 873us/step - loss: 3.9221 - val_loss: 4.5320\n",
      "Epoch 128/200\n",
      "1250/1250 [==============================] - 1s 876us/step - loss: 3.9061 - val_loss: 4.4541\n",
      "Epoch 129/200\n",
      "1250/1250 [==============================] - 1s 868us/step - loss: 3.9230 - val_loss: 4.4733\n",
      "Epoch 130/200\n",
      "1250/1250 [==============================] - 1s 883us/step - loss: 3.9230 - val_loss: 4.6366\n",
      "Epoch 131/200\n",
      "1250/1250 [==============================] - 1s 879us/step - loss: 3.9121 - val_loss: 4.5943\n",
      "Epoch 132/200\n",
      "1250/1250 [==============================] - 1s 872us/step - loss: 3.9276 - val_loss: 4.5574\n",
      "Epoch 133/200\n",
      "1250/1250 [==============================] - 1s 867us/step - loss: 3.9024 - val_loss: 4.5361\n",
      "Epoch 134/200\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 3.9214 - val_loss: 4.5574\n",
      "Epoch 135/200\n",
      "1250/1250 [==============================] - 1s 874us/step - loss: 3.9083 - val_loss: 4.4997\n",
      "Epoch 136/200\n",
      "1250/1250 [==============================] - 1s 875us/step - loss: 3.9257 - val_loss: 4.5607\n",
      "Epoch 137/200\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 3.9225 - val_loss: 4.5379\n",
      "Epoch 138/200\n",
      "1250/1250 [==============================] - 1s 875us/step - loss: 3.9279 - val_loss: 4.5642\n",
      "Epoch 139/200\n",
      "1250/1250 [==============================] - 1s 879us/step - loss: 3.9287 - val_loss: 4.4534\n",
      "Epoch 140/200\n",
      "1250/1250 [==============================] - 1s 875us/step - loss: 3.8965 - val_loss: 4.5780\n",
      "Epoch 141/200\n",
      "1250/1250 [==============================] - 1s 877us/step - loss: 3.9004 - val_loss: 4.8501\n",
      "Epoch 142/200\n",
      "1250/1250 [==============================] - 1s 875us/step - loss: 3.8911 - val_loss: 4.4797\n",
      "Epoch 143/200\n",
      "1250/1250 [==============================] - 1s 872us/step - loss: 3.9189 - val_loss: 4.4898\n",
      "Epoch 144/200\n",
      "1250/1250 [==============================] - 1s 873us/step - loss: 3.9143 - val_loss: 4.5508\n",
      "Epoch 145/200\n",
      "1250/1250 [==============================] - 1s 886us/step - loss: 3.9045 - val_loss: 4.7480\n",
      "Epoch 146/200\n",
      "1250/1250 [==============================] - 1s 879us/step - loss: 3.8929 - val_loss: 4.4648\n",
      "Epoch 147/200\n",
      "1250/1250 [==============================] - 1s 879us/step - loss: 3.8843 - val_loss: 4.4867\n",
      "Epoch 148/200\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 3.8853 - val_loss: 4.7081\n",
      "Epoch 149/200\n",
      "1250/1250 [==============================] - 1s 886us/step - loss: 3.8717 - val_loss: 4.5629\n",
      "Epoch 150/200\n",
      "1250/1250 [==============================] - 1s 877us/step - loss: 3.8981 - val_loss: 4.4758\n",
      "Epoch 151/200\n",
      "1250/1250 [==============================] - 1s 886us/step - loss: 3.8937 - val_loss: 4.4606\n",
      "Epoch 152/200\n",
      "1250/1250 [==============================] - 1s 881us/step - loss: 3.8840 - val_loss: 4.4137\n",
      "Epoch 153/200\n",
      "1250/1250 [==============================] - 1s 881us/step - loss: 3.9047 - val_loss: 4.4956\n",
      "Epoch 154/200\n",
      "1250/1250 [==============================] - 1s 875us/step - loss: 3.8967 - val_loss: 4.4262\n",
      "Epoch 155/200\n",
      "1250/1250 [==============================] - 1s 884us/step - loss: 3.8905 - val_loss: 4.3947\n",
      "Epoch 156/200\n",
      "1250/1250 [==============================] - 1s 877us/step - loss: 3.8960 - val_loss: 4.4251\n",
      "Epoch 157/200\n",
      "1250/1250 [==============================] - 1s 876us/step - loss: 3.8530 - val_loss: 4.6002\n",
      "Epoch 158/200\n",
      "1250/1250 [==============================] - 1s 877us/step - loss: 3.8974 - val_loss: 4.4051\n",
      "Epoch 159/200\n",
      "1250/1250 [==============================] - 1s 883us/step - loss: 3.8726 - val_loss: 4.4967\n",
      "Epoch 160/200\n",
      "1250/1250 [==============================] - 1s 879us/step - loss: 3.8659 - val_loss: 4.3811\n",
      "Epoch 161/200\n",
      "1250/1250 [==============================] - 1s 881us/step - loss: 3.8757 - val_loss: 4.3653\n",
      "Epoch 162/200\n",
      "1250/1250 [==============================] - 1s 877us/step - loss: 3.8694 - val_loss: 4.4056\n",
      "Epoch 163/200\n",
      "1250/1250 [==============================] - 1s 880us/step - loss: 3.8554 - val_loss: 4.3932\n",
      "Epoch 164/200\n",
      "1250/1250 [==============================] - 1s 871us/step - loss: 3.8764 - val_loss: 4.5192\n",
      "Epoch 165/200\n",
      "1250/1250 [==============================] - 1s 880us/step - loss: 3.8751 - val_loss: 4.3843\n",
      "Epoch 166/200\n",
      "1250/1250 [==============================] - 1s 880us/step - loss: 3.8581 - val_loss: 4.2969\n",
      "Epoch 167/200\n",
      "1250/1250 [==============================] - 1s 885us/step - loss: 3.8388 - val_loss: 4.5083\n",
      "Epoch 168/200\n",
      "1250/1250 [==============================] - 1s 882us/step - loss: 3.8707 - val_loss: 4.3965\n",
      "Epoch 169/200\n",
      "1250/1250 [==============================] - 1s 877us/step - loss: 3.8466 - val_loss: 4.3544\n",
      "Epoch 170/200\n",
      "1250/1250 [==============================] - 1s 881us/step - loss: 3.8473 - val_loss: 4.3224\n",
      "Epoch 171/200\n",
      "1250/1250 [==============================] - 1s 880us/step - loss: 3.8448 - val_loss: 4.5844\n",
      "Epoch 172/200\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 3.8629 - val_loss: 4.4882\n",
      "Epoch 173/200\n",
      "1250/1250 [==============================] - 1s 874us/step - loss: 3.8497 - val_loss: 4.4479\n",
      "Epoch 174/200\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 3.8438 - val_loss: 4.4733\n",
      "Epoch 175/200\n",
      "1250/1250 [==============================] - 1s 875us/step - loss: 3.8747 - val_loss: 4.5099\n",
      "Epoch 176/200\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 3.8664 - val_loss: 4.5642\n",
      "Epoch 177/200\n",
      "1250/1250 [==============================] - 1s 877us/step - loss: 3.8434 - val_loss: 4.3825\n",
      "Epoch 178/200\n",
      "1250/1250 [==============================] - 1s 882us/step - loss: 3.8638 - val_loss: 4.4400\n",
      "Epoch 179/200\n",
      "1250/1250 [==============================] - 1s 875us/step - loss: 3.8791 - val_loss: 4.4710\n",
      "Epoch 180/200\n",
      "1250/1250 [==============================] - 1s 876us/step - loss: 3.8496 - val_loss: 4.5879\n",
      "Epoch 181/200\n",
      "1250/1250 [==============================] - 1s 881us/step - loss: 3.8247 - val_loss: 4.5042\n",
      "Epoch 182/200\n",
      "1250/1250 [==============================] - 1s 879us/step - loss: 3.8538 - val_loss: 4.4375\n",
      "Epoch 183/200\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 3.8678 - val_loss: 4.3621\n",
      "Epoch 184/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 1s 871us/step - loss: 3.8296 - val_loss: 4.4884\n",
      "Epoch 185/200\n",
      "1250/1250 [==============================] - 1s 868us/step - loss: 3.8454 - val_loss: 4.4410\n",
      "Epoch 186/200\n",
      "1250/1250 [==============================] - 1s 869us/step - loss: 3.8675 - val_loss: 4.2829\n",
      "Epoch 187/200\n",
      "1250/1250 [==============================] - 1s 872us/step - loss: 3.8127 - val_loss: 4.3512\n",
      "Epoch 188/200\n",
      "1250/1250 [==============================] - 1s 869us/step - loss: 3.8615 - val_loss: 4.4712\n",
      "Epoch 189/200\n",
      "1250/1250 [==============================] - 1s 870us/step - loss: 3.8386 - val_loss: 4.4788\n",
      "Epoch 190/200\n",
      "1250/1250 [==============================] - 1s 870us/step - loss: 3.8241 - val_loss: 4.4483\n",
      "Epoch 191/200\n",
      "1250/1250 [==============================] - 1s 883us/step - loss: 3.8294 - val_loss: 4.4946\n",
      "Epoch 192/200\n",
      "1250/1250 [==============================] - 1s 873us/step - loss: 3.8343 - val_loss: 4.3509\n",
      "Epoch 193/200\n",
      "1250/1250 [==============================] - 1s 875us/step - loss: 3.8293 - val_loss: 4.5868\n",
      "Epoch 194/200\n",
      "1250/1250 [==============================] - 1s 874us/step - loss: 3.8656 - val_loss: 4.4443\n",
      "Epoch 195/200\n",
      "1250/1250 [==============================] - 1s 875us/step - loss: 3.8345 - val_loss: 4.3774\n",
      "Epoch 196/200\n",
      "1250/1250 [==============================] - 1s 879us/step - loss: 3.8380 - val_loss: 4.4473\n",
      "Epoch 197/200\n",
      "1250/1250 [==============================] - 1s 877us/step - loss: 3.8510 - val_loss: 4.5614\n",
      "Epoch 198/200\n",
      "1250/1250 [==============================] - 1s 872us/step - loss: 3.8295 - val_loss: 4.3796\n",
      "Epoch 199/200\n",
      "1250/1250 [==============================] - 1s 875us/step - loss: 3.7987 - val_loss: 4.3740\n",
      "Epoch 200/200\n",
      "1250/1250 [==============================] - 1s 876us/step - loss: 3.8272 - val_loss: 4.5484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f75f0297410>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = optimizers.Adamax(learning_rate=0.004)\n",
    "vae.compile(optimizer=opt, loss='binary_crossentropy')\n",
    "vae.fit(train_vecs, train_vecs,\n",
    "        epochs=200,\n",
    "        batch_size=8,\n",
    "        shuffle=True,\n",
    "        validation_data=(val_vecs, val_vecs))\n",
    "\n",
    "opt = optimizers.Adamax(learning_rate=0.001)\n",
    "vae.compile(optimizer=opt, loss='binary_crossentropy')\n",
    "vae.fit(train_vecs, train_vecs,\n",
    "        epochs=200,\n",
    "        batch_size=8,\n",
    "        shuffle=True,\n",
    "        validation_data=(val_vecs, val_vecs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MY33WSZ', 'WE83GXF', 'LU19JLV', 'KX41GVE', 'WP91NCV', 'SG47LFF', 'HB17WIY', 'CA89AXN', 'GT57AKA', 'CD77SJT']\n",
      "['MY33WSZ', 'WE83GXI', 'LU19JLV', 'KX41GVE', 'WP91NCV', 'SG47LFF', 'HB47WIY', 'CA89AXN', 'GT57AKA', 'CD77SJT']\n",
      "overall accuracy = 0.737\n",
      "by digit accuracy:\n",
      "    0.995\n",
      "    0.984\n",
      "    0.994\n",
      "    0.991\n",
      "    0.981\n",
      "    0.99\n",
      "    0.792\n"
     ]
    }
   ],
   "source": [
    "decoded_regs = vae.predict(test_vecs)\n",
    "print(test_strs[:10])\n",
    "recovered = [v.recover(x) for x in decoded_regs]\n",
    "print(recovered[:10])\n",
    "acc = np.sum([x == y for x, y in zip(recovered, test_strs)]) / len(test_strs)\n",
    "print('overall accuracy = ' + str(acc))\n",
    "print('by digit accuracy:')\n",
    "for i_digit in range(len(test_strs[0])):\n",
    "    acc = np.sum([x[i_digit] == y[i_digit] for x, y in zip(recovered, test_strs)]) / len(test_strs)\n",
    "    print('    ' + str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use decoder as generator\n",
    "\n",
    "By directly sampling in the latent space, and running through the decoder, one effectively has a data generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HY01ZCT', 'CA01IAY', 'PG11NKA', 'MY01IXW', 'SG21BCJ']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.recover(x) for x in decoder.predict(np.random.random((5, latent_dim)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can do some sampling and look at some stats to see what kind of data comes out. For instance, the number of reg numbers with pair of integers below 50 and above 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg numbers below 50 = 433\n",
      "reg numbers above 50 = 567\n"
     ]
    }
   ],
   "source": [
    "random_vectors = 10 * (np.random.random((1000, latent_dim)) - 0.5)\n",
    "generated_regs = [v.recover(x) for x in decoder.predict(random_vectors)]\n",
    "n_reg_below_50 = len({x for x in generated_regs if int(x[2:4]) < 50})\n",
    "n_reg_above_50 = len({x for x in generated_regs if int(x[2:4]) >= 50})\n",
    "print('reg numbers below 50 = ' + str(n_reg_below_50))\n",
    "print('reg numbers above 50 = ' + str(n_reg_above_50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoid generating any training data\n",
    "\n",
    "There is a chance that a random generation of data might produce one of the actual registration numbers used in training. However, any input registrations are represented as a probability distribution in the latent space; so one could require that any sample vector used in the latent space to be 'sufficiently far' down the tails of all the recorded distributions.\n",
    "\n",
    "The distributions of the training data in the latent space are readily available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.9664675 , -0.38055828, -0.48702085,  0.00808121, -0.02023169,\n",
       "         0.26767403, -0.9605626 , -0.8348456 , -0.2087945 , -0.00560325,\n",
       "         0.19209874, -0.06166378, -0.36049056,  0.3712803 , -0.39906198],\n",
       "       dtype=float32),\n",
       " array([0.5190886 , 0.5172951 , 0.52408266, 1.0009084 , 0.9980573 ,\n",
       "        0.5679686 , 0.5070283 , 0.49884477, 0.46346048, 0.49687105,\n",
       "        0.5390688 , 0.9790228 , 0.5516896 , 0.5181226 , 0.5831158 ],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the distributions for each reg number in the data\n",
    "all_strs = train_strs + val_strs + test_strs\n",
    "all_vecs = np.array([v.vectorize(x) for x in all_strs])\n",
    "means, log_sigmas, _ = encoder.predict(all_vecs)\n",
    "distn_dict = {x: (y,z) for x, y, z in zip(all_strs, means, np.exp(log_sigmas))}  # dict<reg no: (mean, sigma)>\n",
    "distn_dict['YK66BIQ']\n",
    "# for a multivariate distribution ~ N(mean, Var) valued in the latent space, \n",
    "# mean is the first vector below, and Var is a diagonal matrix, the square root\n",
    "# of the diagonal entries being given in the second vector.\n",
    "# If the KL loss function has done its job, the distributions should be close to N(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can access the distributions that contribute to the latent space, that represent numbers below 50:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of distributions with number below 50 = 6103\n"
     ]
    }
   ],
   "source": [
    "distns_below_50 = {k: v for k, v in distn_dict.items() if int(k[2:4]) < 50}\n",
    "print('number of distributions with number below 50 = ' + str(len(distns_below_50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and use these to find only random vectors that are 'sufficiently far away' from the means of those reg numbers with numbers below 50. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_vectors = 10 * (np.random.random((1000, latent_dim)) - 0.5)\n",
    "min_mahalanobis_dists = [np.min([np.linalg.norm(np.abs(x-mu)/sigma) \n",
    "                                 for mu, sigma in distns_below_50.values()])\n",
    "                         for x in random_vectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.090161261305113"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = random_vectors[0]\n",
    "means_mtr = np.array([mu for mu, sigma in distns_below_50.values()])\n",
    "sigma_mtr = np.array([sigma for mu, sigma in distns_below_50.values()])\n",
    "min_mahalanobis np.min(np.linalg.norm((x - means_mtr) / sigma_mtr, axis=1))\n",
    "bob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of filtered random vectors = 669\n"
     ]
    }
   ],
   "source": [
    "mahalanobis_threshold = 1.0\n",
    "filtered_random_vectors = [x for x, y in zip(random_vectors, np.array(min_mahalanobis_dists)/latent_dim) \n",
    "                           if y > mahalanobis_threshold]\n",
    "print('number of filtered random vectors = ' + str(len(filtered_random_vectors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered reg numbers above 50 = 401\n"
     ]
    }
   ],
   "source": [
    "filtered_generated_regs = [v.recover(x) for x in decoder.predict(np.array(filtered_random_vectors))]\n",
    "filtered_n_reg_above_50 = len({x for x in filtered_generated_regs if int(x[2:4]) > 50})\n",
    "print('filtered reg numbers above 50 = ' + str(filtered_n_reg_above_50))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
